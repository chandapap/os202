Top 10 List of Week 03

1. [Linux Scheduler]
    - The Linux scheduler is a priority based scheduler that schedules tasks based upon their static and dynamic priorities. When these priorities are combined they form a task's goodness . Each time the Linux scheduler runs, every task on the run queue is examined and its goodness value is computed. The task with the highest goodness is chosen to run next.
    - When there are cpu bound tasks running in the system, the Linux scheduler may not be called for intervals of up to .40 seconds. This means that the currently running task has the CPU to itself for periods of up to .40 seconds (how long depends upon the task's priority and whether it blocks or not). This is good for throughput because there are few computationally uneccessary context switches. However it can kill interactivity because Linux only reschedules when a task blocks or when the task's dynamic priority (counter) reaches zero. Thus under Linux's default priority based scheduling method, long scheduling latencies can occur.
    - Looking at the scheduling latency in finer detail, the Linux scheduler makes use of a timer that interrupts every 10 msec. This timer erodes the currently running task's dynamic priority (decrements its counter). A task's counter starts out at the same value its priority contains. Once its dynamic priority (counter) has eroded to 0 it is again reset to that of its static priority (priority). It is only after the counter reaches 0 that a call to schedule() is made. Thus a task with the default priority of 20 may run for .200 secs (200 msecs) before any other task in the system gets a chance to run. A task at priority 40 (the highest priority allowed) can run for .400 secs without any scheduling occurring as long as it doesn't block or yield.
    - Linux scheduler has been gone through some big improvements since kernel version 2.4. There were a lot of complaints about the interactivity of the scheduler in kernel 2.4. During this version, the scheduler was implemented with one running queue for all available processors. At every scheduling, this queue was locked and every task on this queue got its timeslice update. This implementation caused poor performance in all aspects. The scheduler algorithm and supporting code went through a large rewrite early in the 2.5 kernel development series. The new scheduler was arisen to achieveO(1 ) run-time regardless number of runnable tasks in the system. To achieve this, each processor has its own running queue. This helps a lot in reducing lock contention. The priority array was introduced which used active array and expired array to keep track running tasks in the system. TheO(1 ) running time is primarily drawn from this new data structure. The scheduler puts all expired processes into expired array. When there is no active process available in active array, it swaps active array with expired array, which makes active array becomes expired array and expired array becomes active array. There were some twists made into this scheduler to optimize further by putting expired task back to active array instead of expired array in some cases.O(1 ) scheduler uses a heuristic calculation to update dynamic priority of tasks based on their interactivity (I/O bound versus CPU bound) The industry was happy with this new scheduler until Con Kolivas introduced his new scheduler named Rotating Staircase Deadline (RSDL) and then later Staircase Deadline (SD). His new schedulers proved the fact that fair scheduling among processes can be achieved without any complex computation. His scheduler was designed to run inO(n ) but its performance exceeded the currentO(1 ) scheduler.
    - The result achieved from SD scheduler surprised all kernel developers and designers. The fair scheduling approach in SD scheduler encouraged Igno Molnar to re-implement the new Linux scheduler named Completely Fair Scheduler (CFS). CFS scheduler was a big improvement over the existing scheduler not only in its performance and interactivity but also in simplifying the scheduling logic and putting more modularized code into the scheduler. CFS scheduler was merged into mainline version 2.6.23. Since then, there have been some minor improvements made to CFS scheduler in some areas such as optimization, load balancing and group scheduling feature.
    
2. [VPN]
    - A virtual private network (VPN) gives you online privacy and anonymity by creating a private network from a public internet connection. VPNs mask your internet protocol (IP) address so your online actions are virtually untraceable. Most important, VPN services establish secure and encrypted connections to provide greater privacy than even a secured Wi-Fi hotspot.
        Why do you need a VPN service?
        - Surfing the web or transacting on an unsecured Wi-Fi network means you could be exposing your private information and browsing habits. That’s why a virtual private network, better known as a VPN, should be a must for anyone concerned about their online security and privacy. Think about all the times you’ve been on the go, reading emails while in line at the coffee shop, or checking your bank account while waiting at the doctor’s office. Unless you were logged into a private Wi-Fi network that requires a password, any data transmitted during your online session could be vulnerable to eavesdropping by strangers using the same network. The encryption and anonymity that a VPN provides helps protect your online activities: sending emails, shopping online, or paying bills. VPNs also help keep your web browsing anonymous.
        How a VPN protects your IP address and privacy
        - VPNs essentially create a data tunnel between your local network and an exit node in another location, which could be thousands of miles away, making it seem as if you’re in another place. This benefit allows online freedom, or the ability to access your favorite apps and websites while on the go. Here’s a closer look at how a virtual private network works. VPNs use encryption to scramble data when it’s sent over a Wi-Fi network. Encryption makes the data unreadable. Data security is especially important when using a public Wi-Fi network, because it prevents anyone else on the network from eavesdropping on your internet activity. There’s another side to privacy. Without a VPN, your internet service provider can know your entire browsing history. With a VPN, your search history is hidden. That’s because your web activity will be associated with the VPN server’s IP address, not yours. A VPN service provider may have servers all over the world. That means your search activity could appear to originate at any one of them. Keep in mind, search engines also track your search history, but they’ll associate that information with an IP address that’s not yours. Again, your VPN will keep your online activity private.
    
3. [Email Hijacking / Email Hacking]
    - Email Hijacking, or email hacking, is a widespread menace nowadays. It works by using the following three techniques which are email spoofing, social         engineering tools, or inserting viruses in a user computer.
        How to detect if your email has been hijacked?
        - The recipients of spam emails include a bunch of people you know.
        - You try to access your account and the password no longer works.
        - You try to access the “Forgot Password” link and it does not go to the expected email.
        - Your Sent Items folder contains a bunch of spams you are not aware of sending.
        Quick tips
        - In case you think that your email got hijacked, then you need to take the following actions −
        - Change the passwords immediately.
        - Notify your friends not to open links that they receive from your email account.
        - Contact the authorities and report that your account has been hacked.
        - Install a good antivirus on your computer and update it.
        - Set up double authentication password if it is supported.
4. GnuPG
    - GnuPG is a complete and free implementation of the OpenPGP standard as defined by RFC4880 (also known as PGP). GnuPG allows you to encrypt and sign your data and communications; it features a versatile key management system, along with access modules for all kinds of public key directories. GnuPG, also known as GPG, is a command line tool with features for easy integration with other applications. 
    - A wealth of frontend applications and libraries are available. GnuPG also provides support for S/MIME and Secure Shell (ssh). Since its introduction in 1997, GnuPG is Free Software (meaning that it respects your freedom). It can be freely used, modified and distributed under the terms of the GNU General Public License . The current version of GnuPG is 2.2.23. See the download page for other maintained versions. Gpg4win is a Windows version of GnuPG featuring a context menu tool, a crypto manager, and an Outlook plugin to send and receive standard PGP/MIME mails. The current version of Gpg4win is 3.1.13.

5. File Concept
    - Computers can store information on various storage media, such as magnetic disks, magnetic tapes, and optical disks.  So that the computer system will be convenient to use, the operating system provides a uniform logical view of information storage. The operating system abstracts from the physical properties of its storage devices to define a logical storage unit, the file. Files are mapped by the operating system onto physical devices. These storage devices are usually nonvolatile, so the contents are persistent through power failures and system reboots.
    - A file is a named collection of related information that is recorded on secondary storage. From a user's perspective, a tile is the smallest allotment of logical secondary storage; that is, data cannot be written to secondary storage unless they are within a file. Commonly, files represent programs (both source and object forms) and data. Data files may be numeric, alphabetic, alphanumeric, or binary.Files may be free form, such as text files, or may be formatted rigidly. In general, a file is a sequence of bits, bytes, lines, or records, the meaning of which is defined by the file's creator and user. 
    - The concept of a file is thus extremely general. The information in a file is defined by its creator. Many different types of information may be stored in a file—source programs, object programs, executable programs, numeric data, text, payroll records, graphic images, sound recordings, and so on. A file has a certain defined structure, which depends on its type. A text file is a sequence of characters organized into lines (and possibly pages). A source file is a sequence of subroutines and functions, each of which is further organized as declarations followed by executable statements. An object file is a sequence of bytes organized into blocks understandable by the system's linker. An executable file is a series of code sections that the loader can bring into memory and execute.

6. Access Method
    - Most of the operating systems access the file sequentially. In other words, we can say that most of the files need to be accessed sequentially by the operating system. In sequential access, the OS read the file word by word. A pointer is maintained which initially points to the base address of the file. If the user wants to read first word of the file then the pointer provides that word to the user and increases its value by 1 word. This process continues till the end of the file. Modern word systems do provide the concept of direct access and indexed access but the most used method is sequential access due to the fact that most of the files such as text files, audio files, video files, etc need to be sequentially accessed.
      1. Direct Access
        The Direct Access is mostly required in the case of database systems. In most of the cases, we need filtered information from the database. The sequential access can be very slow and inefficient in such cases. Suppose every block of the storage stores 4 records and we know that the record we needed is stored in 10th block. In that case, the sequential access will not be implemented because it will traverse all the blocks in order to access the needed record. Direct access will give the required result despite of the fact that the operating system has to perform some complex tasks such as determining the desired block number. However, that is generally implemented in database applications.
      2. Sequential Access
        It is the simplest access method. Information in the file is processed in order, one record after the other. This mode of access is by far the most common; for example, editor and compiler usually access the file in this fashion.  Read and write make up the bulk of the operation on a file. A read operation -read next- read the next position of the file and automatically advance a file pointer, which keeps track I/O location. Similarly, for the writewrite next append to the end of the file and advance to the newly written material. 
      3. Index sequential method
        It is the other method of accessing a file which is built on the top of the direct access method. These methods construct an index for the file. The index, like an index in the back of a book, contains the pointer to the various blocks. To find a record in the file, we first search the index and then by the help of pointer we access the file directly. 
    
7. Disk Defragmentation
    - Defragmentation, also known as “defrag” or “defragging” is the process of reorganizing the data stored on the hard drive so that related pieces of data are put back together, all lined up in a continuous fashion.  You could say that defragmentation is like cleaning house for your server or PC, it picks up all of the pieces of data that are spread across your hard drive and puts them back together again. Why is defragmentation important? Because every computer suffers from the constant growth of fragmentation and if you don’t “clean house”, your servers and PCs suffer.
      How Fragmentation Occurs
      - Disk fragmentation occurs when a file is broken up into pieces to fit on the disk. Because files are constantly being written, deleted and resized, fragmentation is a natural occurrence. When a file is spread out over several locations, it takes longer to read and write. But the effects of fragmentation are far more widespread.
      Effects of Fragmentation on Computer Performance
      - Many users blame computer performance problems on the operating system or simply think their computer is “old”, when disk fragmentation is most often the real culprit. The weakest link in computer performance is the disk. It is at least 100,000 times slower than RAM and over 2 million times slower than the CPU. In terms of computer performance, the disk is the primary bottleneck. File fragmentation directly affects the access and write speed of that disk, steadily corrupting computer performance to unviable levels. Because all computers suffer from fragmentation, this is a critical issue to resolve.

8. Partition
    - When referring to a computer hard drive, a disk partition or partition is a section of the hard drive that is separated from other segments. Partitions enable users to divide a physical disk into logical sections. For example, allowing multiple operating systems to run on the same device. With older file allocation tables, such as FAT 16, creating smaller partitions allows a computer hard drive to run more efficiently and save more disk space. However, with new file allocation tables, such as FAT32, this is no longer the case.
    - There are also several partition types. Below is a listing of some of these partitions with a brief description. Some of these partitions may not be available in your partition utility.
        Partition	                    Description
        AIX partition (boot)	        A partition used with the AIX operating system.
        Boot partition	              As defined by Microsoft, a boot partition is a partition that contains the files required for a system startup. Also see: System partition
        BSD/OS partition (OpenBSD)	  A partition used with the BSD operating system.
        DOS partition (12/16-bit)	A partition used with older versions of MS-DOS.
        DOS extended partition	      A partition that is extended from one or more of the original MS-DOS partitions.
        DRDOS (hHidden)	              A partition used with the DR. DOS operating system.
        Extended partition	          A partition that is extended from one or more of the primary partitions.
        Hibernation partition	        A partition used with older hibernation programs.
        HPFS partition (OS/2 IFS)	    An HPFS partition used with IBM OS/2 and Microsoft NT 3.x
        Linux 	                      A partition used with various variants of the Linux operating systems.
        MINIX	                        A partition used with the MINIX operating system.
        NON-DOS                       A NON-DOS partition indicates a partition that is not native to the Microsoft operating system. For example, this could be a Linux partition.
        NEC DOS	                      A partition used with the old NEC DOS variant.
        NEXTSTEP	                    A partition used with the NeXTSTEP operating system.
        Novell NetWare	              A partition used with the Novell NetWare operating system.
        NTFS	                        A partition used with Microsoft Windows NT 4.x, Windows 2000 and Windows XP.
        Partition Magic (PowerQuest)	A partition created using the Partition Magic utility by PowerQuest.
        PC-ARMOUR	                    A partition created by the PC ARMOUR security utility. When created this partition is commonly protected by a password.
        Primary	                      In a Microsoft operating system, the Primary Partition refers to the main or first partition used for the Microsoft operating system.
        Solaris X86	                  A partition used with the Sun Solaris X86 platform operating system.
        System partition	            As defined by Microsoft, a system partition is a partition that contains the system32 directory. Also see: boot partition.
        Tandy DOS	                    A partition used with the old Tandy DOS variant.
        Unix System V (SCO, IRIX, ISC, Unix, UnixWare, etc...)	A partition used with various Unix operating systems.
        VMware (VMware Swap)	        A partition used by VMware.
        XENIX (XENIX /usr)	          A partition used with the Xenix operating system.

9. Virtual FS
    - The Virtual File System (also known as the Virtual Filesystem Switch) is the software layer in the kernel that provides the filesystem interface to userspace programs. It also provides an abstraction within the kernel which allows different filesystem implementations to coexist. VFS system calls open(2), stat(2), read(2), write(2), chmod(2) and so on are called from a process context. Filesystem locking is described in the document Documentation/filesystems/locking.rst.
      Directory Entry Cache (dcache)
      - The VFS implements the open(2), stat(2), chmod(2), and similar system calls. The pathname argument that is passed to them is used by the VFS to search through the directory entry cache (also known as the dentry cache or dcache). This provides a very fast look-up mechanism to translate a pathname (filename) into a specific dentry. Dentries live in RAM and are never saved to disc: they exist only for performance.
        The dentry cache is meant to be a view into your entire filespace. As most computers cannot fit all dentries in the RAM at the same time, some bits of the cache are missing. In order to resolve your pathname into a dentry, the VFS may have to resort to creating dentries along the way, and then loading the inode. This is done by looking up the inode.
      The Inode Object
      - An individual dentry usually has a pointer to an inode. Inodes are filesystem objects such as regular files, directories, FIFOs and other beasts. They live either on the disc (for block device filesystems) or in the memory (for pseudo filesystems). Inodes that live on the disc are copied into the memory when required and changes to the inode are written back to disc. A single inode can be pointed to by multiple dentries (hard links, for example, do this).
        To look up an inode requires that the VFS calls the lookup() method of the parent directory inode. This method is installed by the specific filesystem implementation that the inode lives in. Once the VFS has the required dentry (and hence the inode), we can do all those boring things like open(2) the file, or stat(2) it to peek at the inode data. The stat(2) operation is fairly simple: once the VFS has the dentry, it peeks at the inode data and passes some of it back to userspace.
      The File Object
      - Opening a file requires another operation: allocation of a file structure (this is the kernel-side implementation of file descriptors). The freshly allocated file structure is initialized with a pointer to the dentry and a set of file operation member functions. These are taken from the inode data. The open() file method is then called so the specific filesystem implementation can do its work. You can see that this is another switch performed by the VFS. The file structure is placed into the file descriptor table for the process.
        Reading, writing and closing files (and other assorted VFS operations) is done by using the userspace file descriptor to grab the appropriate file structure, and then calling the required file structure method to do whatever is required. For as long as the file is open, it keeps the dentry in use, which in turn means that the VFS inode is still in use.

10. Directory Structure
    - The directory structure is the organization of files into a hierarchy of folders. It should be stable and scalable; it should not fundamentally change, only be added to. Computers have used the folder metaphor for decades as a way to help users keep track of where something can be found. Folders are very limited as an organizational structure, however. There must be one top-level organizational construct, which can only be subdivided in a limited way before the system becomes too cumbersome and breaks down. Which is most important to divide by: date, client, project, subject matter, rating, or usage? Furthermore, information that is dependent on folder structure is very fragile. If you remove an image from a folder that designates what that image is, that content information can be lost; however, folders provide an ideal tool for managing the data itself. We suggest you should use folders principally for storage, rather than for organization.
      By storage, we mean containing the images, putting them away, moving them around, and other handling issues, distinct from the organization of the images that is best accomplished by using metadata. In the physical world, storage and organizational structure are often inseparable. In the digital world, we are not so constrained. We’ll see how you can use folders in a simple, straightforward way to stack files up so that you can back them up, validate them, and restore them in the event of a problem.  This does not mean that folder naming is irrelevant as a content-organizing tool; it means that content organizing is a secondary job.
